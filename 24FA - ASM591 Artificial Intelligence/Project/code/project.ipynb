{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TJ Wiegman  \n",
    "ASM 591 AI  \n",
    "Final Project  \n",
    "2024-12-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Enable GPU acceleration\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "BATCH = 20\n",
    "\n",
    "class VisOdoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Convolutional layers to learn spatial features\n",
    "        self.conv1 = nn.Conv2d(4, 5, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(5, 3, kernel_size=3, padding=3, dilation=2)\n",
    "        self.conv3 = nn.Conv2d(3, 1, kernel_size=5, padding=2)\n",
    "        \n",
    "        # LSTM network to learn temporal features\n",
    "        self.rnn1 = nn.LSTM(input_size=256,\n",
    "                            hidden_size=8, num_layers=4,\n",
    "                            bidirectional=False, # should be monotonic\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # And a few FC layers to tie it all together\n",
    "        self.fc1 = nn.Linear(64, 16)\n",
    "        self.fc2 = nn.Linear(16, BATCH)\n",
    "        \n",
    "    def forward(self, x):         # input [B x 4 x 256 x 256]\n",
    "        x = F.relu(self.conv1(x)) # shape [B x 5 x 256 x 256]\n",
    "        x = F.avg_pool2d(x, 2)    # shape [B x 5 x 128 x 128]\n",
    "        x = F.relu(self.conv2(x)) # shape [B x 3 x 128 x 128]\n",
    "        x = F.max_pool2d(x, 4)    # shape [B x 3 x  32 x  32]\n",
    "        x = F.relu(self.conv3(x)) # shape [B x 1 x  32 x  32]\n",
    "        x = F.avg_pool2d(x, 2)    # shape [B x 1 x  16 x  16]\n",
    "        x = x.reshape(-1, 256)    # shape [B x 256]\n",
    "        \n",
    "        _, (x, c) = self.rnn1(x)  # pull both hidden and cell states\n",
    "        x = x.reshape(-1, 32) # B x 32 hidden state\n",
    "        c = c.reshape(-1, 32) # B x 32 cell state\n",
    "        x = torch.concat((x,c))\n",
    "        x = F.relu(x.reshape(-1, 64))   # [B x 64]\n",
    "        x = F.relu(self.fc1(x))   # shape [B x 16]\n",
    "        x = F.relu(self.fc2(x))   # shape [BATCH]\n",
    "        return x.reshape(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "def stamp(dt):\n",
    "    '''Converts a datetime into a string suitable for indexing'''\n",
    "    # minute = dt.minute - (dt.minute % 10)\n",
    "    # return f\"{dt.date()}_{dt.hour:02}:{minute:02}\"\n",
    "    return f\"{dt.date()}_{dt.hour:02}:{dt.minute:02}\"\n",
    "\n",
    "def unstamp(st):\n",
    "    '''Converts a `stamp` string back into a datetime'''\n",
    "    date, time = st.split(\"_\")\n",
    "    yy,mm,dd = date.split(\"-\")\n",
    "    hh,mn = time.split(\":\")\n",
    "    #hh,mn = time, 0\n",
    "    return datetime.datetime(\n",
    "        year=int(yy), month=int(mm), day=int(dd),\n",
    "        hour=int(hh), minute=int(mn)\n",
    "    )\n",
    "\n",
    "def roll_avg(frame, new):\n",
    "    N = frame[1] + 1\n",
    "    output = frame[0]*(1 - 1/N) + new/N\n",
    "    return output, N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "# Import depth feature extraction model\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Import DAv2 code\n",
    "import sys\n",
    "sys.path.append(\"./\")\n",
    "from Depth_Anything_V2.depth_anything_v2.dpt import DepthAnythingV2\n",
    "\n",
    "# From https://github.com/DepthAnything/Depth-Anything-V2#use-our-models\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "\n",
    "encoder = 'vitl' # or 'vits', 'vitb', 'vitg'\n",
    "\n",
    "depth_model = DepthAnythingV2(**model_configs[encoder])\n",
    "depth_model.load_state_dict(torch.load(f\"Depth_Anything_V2/checkpoints/depth_anything_v2_{encoder}.pth\", weights_only=True, map_location='cpu'))\n",
    "depth_model = depth_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, time\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "class FileVideoStream:\n",
    "    # queue up frames in the background for faster playback\n",
    "    # adapted from https://pyimagesearch.com/2017/02/06/faster-video-file-fps-with-cv2-videocapture-and-opencv/\n",
    "    def __init__(self, path, queueSize=60):\n",
    "        self.stream = cv2.VideoCapture(path)\n",
    "        self.stopped = False\n",
    "        self.Q = Queue(maxsize=queueSize)\n",
    "    \n",
    "    def more(self): return self.Q.qsize() > 0\n",
    "    def read(self): return self.Q.get()\n",
    "    def stop(self): self.stopped = True\n",
    "\n",
    "    def update(self):\n",
    "        while True:\n",
    "            if self.stopped: return\n",
    "            if not self.Q.full():\n",
    "                (grabbed,frame) = self.stream.read()\n",
    "                if not grabbed:\n",
    "                    self.stop()\n",
    "                    return\n",
    "                self.Q.put(frame)\n",
    "\n",
    "    def start(self):\n",
    "        t = Thread(target=self.update, args=())\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        time.sleep(1.0) # give the queue a second to fill a bit\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from pynviread import NvidiaReader  # fast but unstable :(\n",
    "import numpy as np\n",
    "import cv2, json\n",
    "\n",
    "class VideoSet(Dataset):\n",
    "    def __init__(self, video_paths):\n",
    "        self.jsons = []\n",
    "        self.sources = []\n",
    "        self.n_frames = 0\n",
    "        self.videos = {}\n",
    "        \n",
    "        for path in video_paths:\n",
    "            # Get metadata from JSON\n",
    "            name, ext = path.split(\".\")\n",
    "            assert ext.upper() == \"MP4\"\n",
    "            jfile = name + \".json\"\n",
    "            with open(jfile) as file:\n",
    "                jdata = json.load(file)\n",
    "            \n",
    "            # Calculate frame numbers\n",
    "            startFrame = self.n_frames\n",
    "            self.n_frames += jdata[-1][\"frame\"] + 1 # because zero indexed\n",
    "            endFrame = self.n_frames\n",
    "            \n",
    "            # Save data to self\n",
    "            self.sources.append((startFrame, endFrame, path))\n",
    "            self.jsons.append(jdata)\n",
    "            \n",
    "    def source_lookup(self, idx):\n",
    "        i = 0\n",
    "        for start, end, _ in self.sources:\n",
    "            if idx >= start and idx < end:\n",
    "                return i\n",
    "            i += 1\n",
    "        raise IndexError(f\"Index {idx} not found in {self.sources}\")\n",
    "    \n",
    "    def load_video(self, path):\n",
    "        self.videos[path] = FileVideoStream(path).start()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_frames\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Lookup correct source for idx\n",
    "        i = self.source_lookup(idx)\n",
    "        start, _, path = self.sources[i]\n",
    "        fidx = idx - start\n",
    "        if path not in self.videos:\n",
    "            self.load_video(path) # lazy loading\n",
    "        \n",
    "        # Get visual data\n",
    "        frame = self.videos[path].read() # shape H x W x 3\n",
    "        depth = np.expand_dims(depth_model.infer_image(frame), -1) # shape H x W x 1\n",
    "        frame = np.concat([x for x in [frame, depth]], axis = 2)\n",
    "        frame = cv2.resize(src=frame, dsize=(256,256), interpolation=cv2.INTER_AREA)\n",
    "        frame = frame.transpose(2,0,1) # because cv2 and pytorch don't agree on axis order\n",
    "        frame = torch.tensor(frame, dtype=torch.float)\n",
    "        \n",
    "        # Get timestamp\n",
    "        time = stamp(parser.isoparse(self.jsons[i][fidx][\"gps_time\"]))\n",
    "        return frame, time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_videos = [\"/mnt/nas/2024/Tractor02/2024-09-17_175407.MP4\", \"/mnt/nas/2024/Tractor04/2024-09-10_164521.MP4\"]\n",
    "# test_set = VideoSet(test_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground-Truth Data Import\n",
    "canbus = {\n",
    "    \"apalis1\": \"/home/tjw/Documents/apalis24/2024_apalis1.sql\",\n",
    "    \"apalis2\": \"/home/tjw/Documents/apalis24/2024_apalis2.sql\",\n",
    "    \"apalis3\": \"/home/tjw/Documents/apalis24/2024_apalis3.sql\",\n",
    "    \"apalis4\": \"/home/tjw/Documents/apalis24/2024_apalis4.sql\"\n",
    "}\n",
    "\n",
    "isoblue = {\n",
    "    # really Tractor02\n",
    "    \"Tractor01\": {\n",
    "        \"apalis1\": {\"start\": datetime.datetime(2024,1,1),\n",
    "                    \"end\": datetime.datetime(2024,6,17)},\n",
    "        \"apalis3\": {\"start\": datetime.datetime(2024,6,17),\n",
    "                    \"end\": datetime.datetime(2024,10,31)}\n",
    "    },\n",
    "\n",
    "    # really Tractor03\n",
    "    \"Tractor02\": {\n",
    "        \"apalis2\": {\"start\": datetime.datetime(2024,1,1),\n",
    "                    \"end\": datetime.datetime(2024,8,22)},\n",
    "        \"apalis4\": {\"start\": datetime.datetime(2024,1,1),\n",
    "                    \"end\": datetime.datetime(2024,10,31)}\n",
    "    },\n",
    "\n",
    "    # really Tractor04\n",
    "    \"Tractor03\": {\n",
    "        \"apalis3\": {\"start\": datetime.datetime(2024,1,1),\n",
    "                    \"end\": datetime.datetime(2024,6,17)},\n",
    "        \"apalis1\": {\"start\": datetime.datetime(2024,6,17),\n",
    "                    \"end\": datetime.datetime(2024,8,22)},\n",
    "        \"apalis2\": {\"start\": datetime.datetime(2024,8,22),\n",
    "                    \"end\": datetime.datetime(2024,10,31)}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get line totals for DB dumps\n",
    "import subprocess\n",
    "\n",
    "line_total = {}\n",
    "\n",
    "for db in canbus:\n",
    "    line_total[db] = int(subprocess.check_output([\"wc\", \"-l\", canbus[db]]).decode().split()[0])\n",
    "\n",
    "# line_total = {'apalis1': 274850703, 'apalis2': 350961387, 'apalis3': 99362332, 'apalis4': 33419054}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tractor01/apalis1] Finished with apalis1 in 128s                                                  \n",
      "[Tractor01/apalis3] Finished with apalis3 in 46s                                                  \n",
      "**[Tractor01] Found a total of 6267 speeds\n",
      "[Tractor02/apalis2] Finished with apalis2 in 162s                                                  \n",
      "[Tractor02/apalis4] Finished with apalis4 in 15s                                                  \n",
      "**[Tractor02] Found a total of 1007 speeds\n",
      "[Tractor03/apalis3] Finished with apalis3 in 45s                                                  \n",
      "[Tractor03/apalis1] Finished with apalis1 in 122s                                                  \n",
      "[Tractor03/apalis2] Finished with apalis2 in 167s                                                  \n",
      "**[Tractor03] Found a total of 11693 speeds\n"
     ]
    }
   ],
   "source": [
    "# Read Isoblue CAN data\n",
    "import time\n",
    "from dateutil import parser\n",
    "MAGIC = 1/900 # empirical scaling factor for CAN bus ground speed to get m/s\n",
    "BAR_W = 49\n",
    "\n",
    "gt_speed = {}\n",
    "\n",
    "for tractor in isoblue:\n",
    "    gt_speed[tractor] = {}\n",
    "\n",
    "    for db in isoblue[tractor]:\n",
    "        stime = time.time()\n",
    "        start = isoblue[tractor][db][\"start\"]\n",
    "        end = isoblue[tractor][db][\"end\"]\n",
    "\n",
    "        with open(canbus[db]) as file:\n",
    "            i = 0\n",
    "            for line in file:\n",
    "                if \"can0\" in line:\n",
    "                    l = line.split(\"\\t\")\n",
    "                    arb = l[2]\n",
    "                    if arb[2:6] == \"FEF1\": # vehicle ground speed PGN\n",
    "                        \n",
    "                        # Get datetime of line\n",
    "                        timestamp = l[0].split(\".\")[0]\n",
    "                        if \"+\" in timestamp:\n",
    "                            timestamp = timestamp.split(\"+\")[0]\n",
    "                        dt = parser.isoparse(timestamp)\n",
    "                        \n",
    "                        # Check that line is within parameters of tractor\n",
    "                        if dt >= start and dt < end:\n",
    "                            hex_data = l[3]\n",
    "                            speed = MAGIC * (256*int(hex_data[4:6], 16) + \n",
    "                                             int(hex_data[6:8], 16))\n",
    "                            \n",
    "                            # Save data to output\n",
    "                            st = stamp(dt)\n",
    "                            if st not in gt_speed[tractor]:\n",
    "                                gt_speed[tractor][st] = (speed, 1)\n",
    "                            else:\n",
    "                                gt_speed[tractor][st] = roll_avg(\n",
    "                                    gt_speed[tractor][st], speed\n",
    "                                )\n",
    "            \n",
    "                # Print progress\n",
    "                i += 1\n",
    "                if i % 250_000 == 0:\n",
    "                    y = time.time() - stime\n",
    "                    x = i/line_total[db]\n",
    "                    pct = round(BAR_W*x)\n",
    "                    bar = \"=\"*pct\n",
    "                    space = \" \"*(BAR_W-pct)\n",
    "                    print(f\"[{tractor}/{db}]: |{bar}>{space}| ~{round((y/x)-y)}s remain...   \", end=\"\\r\")\n",
    "        print(f\"[{tractor}/{db}] Finished with {db} in {round(time.time() - stime)}s {' '*BAR_W}\")\n",
    "    print(f\"**[{tractor}] Found a total of {len(gt_speed[tractor])} speeds\")\n",
    "\n",
    "# Estimated total runtime: 10-15 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# with open(\"gt_data.json\", \"w\") as file:\n",
    "#     json.dump(gt_speed, file)\n",
    "\n",
    "with open(\"gt_data.json\", \"r\") as file:\n",
    "    gt_speed = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos = [\n",
    "    '/mnt/nas/2024/Tractor02/2024-06-18_083656.MP4',\n",
    "    #'/mnt/nas/2024/Tractor02/2024-06-18_120818.MP4',\n",
    "    # '/mnt/nas/2024/Tractor02/2024-06-19_083249.MP4',\n",
    "    # '/mnt/nas/2024/Tractor02/2024-06-19_095128.MP4',\n",
    "    # '/mnt/nas/2024/Tractor02/2024-06-19_154328.MP4',\n",
    "    # '/mnt/nas/2024/Tractor02/2024-09-10_073820.MP4',\n",
    "    # '/mnt/nas/2024/Tractor02/2024-09-11_082744.MP4'\n",
    "]\n",
    "\n",
    "testing_videos = [\n",
    "    # '/mnt/nas/2024/Tractor02/2024-06-18_083656.MP4',\n",
    "    # '/mnt/nas/2024/Tractor02/2024-06-18_120818.MP4',\n",
    "    '/mnt/nas/2024/Tractor02/2024-06-19_083249.MP4',\n",
    "    # '/mnt/nas/2024/Tractor02/2024-06-19_095128.MP4',\n",
    "    # '/mnt/nas/2024/Tractor02/2024-06-19_154328.MP4',\n",
    "    # '/mnt/nas/2024/Tractor02/2024-09-10_073820.MP4',\n",
    "    # '/mnt/nas/2024/Tractor02/2024-09-11_082744.MP4'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "train_set = VideoSet(training_videos)\n",
    "test_set = VideoSet(testing_videos)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_set,\n",
    "    batch_size = BATCH,\n",
    "    shuffle = False # loads videos sequentially == faster reads\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_set,\n",
    "    batch_size = BATCH,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training function\n",
    "def train(epoch, model, device, optimizer, data_loader, loss_function, gt_data):\n",
    "    # Prepare model\n",
    "    model = model.to(device)\n",
    "    model = model.train()\n",
    "    \n",
    "    try:\n",
    "        for batch_idx, (frame, time) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            frame = frame.to(device)\n",
    "\n",
    "            # Get ground truth for comparison\n",
    "            preds = []\n",
    "            for st in time:\n",
    "                if st in gt_data:\n",
    "                    preds.append(gt_data[st][0])\n",
    "                else:\n",
    "                    preds.append(np.nan)\n",
    "            y = np.array(preds).reshape(-1)\n",
    "            y = torch.tensor(y, dtype=torch.float).to(device)\n",
    "\n",
    "            # Calculate and record output & loss\n",
    "            output = model(frame)\n",
    "\n",
    "            # Ensure dimensions match:\n",
    "            if output.shape == y.shape:\n",
    "                loss = loss_function(output, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            else:\n",
    "                print(f\"Dimension mismatch between prediction {output} and ground truth {y}\")\n",
    "                print(f\" └─> Skipping loss calculation for {time}\")\n",
    "\n",
    "            # Periodically report on training progress\n",
    "            print(f\"\\rEpoch {epoch}: Training {batch_idx*BATCH}/{len(data_loader.dataset)} \" + \n",
    "                  f\"(Loss: {loss.item():02.4})\", end=\" \"*10)\n",
    "\n",
    "        print(f\"\\rEpoch {epoch}: Trained {len(data_loader.dataset)}/{len(data_loader.dataset)} \" + \n",
    "                  f\"(Loss: {loss.item():02.4})\" + \" \"*10)\n",
    "        return (loss, None)\n",
    "    except KeyboardInterrupt:\n",
    "        return (loss, KeyboardInterrupt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create testing function\n",
    "def test(epoch, model, device, data_loader, loss_function, gt_data):\n",
    "    # Prepare model and data\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    test_loss = []\n",
    "    map = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (frame, time) in enumerate(data_loader):\n",
    "            # Load data into `device`\n",
    "            frame = frame.to(device)\n",
    "            \n",
    "            # Get ground truth for comparison\n",
    "            preds = []\n",
    "            for st in time:\n",
    "                if st in gt_data:\n",
    "                    preds.append(gt_data[st][0])\n",
    "                else:\n",
    "                    preds.append(np.nan)\n",
    "            y = np.array(preds).reshape(-1)\n",
    "            y = torch.tensor(y, dtype=torch.float).to(device)\n",
    "            \n",
    "            # Calculate loss and accuracy\n",
    "            output = model(frame)\n",
    "            \n",
    "            # Ensure dimensions match\n",
    "            if output.shape == y.shape:\n",
    "                test_loss.append(loss_function(output, y).item())\n",
    "                map.append(torch.mean(torch.abs((output - y) / y)) * 100)\n",
    "            else:\n",
    "                print(f\"Dimension mismatch between prediction {output} and ground truth {y}\")\n",
    "                print(f\" --> Skipping loss calculation for {time}\")\n",
    "            \n",
    "            # Periodically report on testing progress\n",
    "            print(f\"\\rEpoch {epoch}: Testing {batch_idx*BATCH}/{len(data_loader.dataset)}, estimated MAPE {torch.mean(torch.tensor(map)):02.4}%\", end=\" \"*10)\n",
    "        print(f\"\\rEpoch {epoch}: Testing {len(data_loader.dataset)}/{len(data_loader.dataset)}\" + \" \"*10)\n",
    "    \n",
    "    # Report results\n",
    "    test_loss = torch.mean(torch.tensor(test_loss))\n",
    "    accuracy = torch.tensor(map)\n",
    "    print(f\"Test Result, epoch {epoch}: Avg loss {test_loss:04.4}, MAPE {torch.mean(accuracy):02.4}%\")\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /home/tjw/Downloads/asm591ai_model.tar, trained to epoch 1 with loss 0.6839\n"
     ]
    }
   ],
   "source": [
    "# Set up training/testing loop\n",
    "import os\n",
    "\n",
    "MAX_EPOCHS = 10\n",
    "model = VisOdoNet()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "model_checkpoint = \"/home/tjw/Downloads/asm591ai_model.tar\"\n",
    "if os.path.exists(model_checkpoint):\n",
    "    checkpoint = torch.load(model_checkpoint, weights_only=True)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    min_epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Loaded model from {model_checkpoint}, trained to epoch {min_epoch} with loss {loss.item():0.4f}\")\n",
    "else:\n",
    "    min_epoch = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Test\n",
    "for epoch in range(min_epoch, MAX_EPOCHS+1):\n",
    "    loss = train(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        loss_function=F.mse_loss,\n",
    "        gt_data=gt_speed[\"Tractor01\"]\n",
    "    )\n",
    "    \n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss[0],\n",
    "        },\n",
    "        model_checkpoint)\n",
    "    print(f\"Saved model to {model_checkpoint}\")\n",
    "    \n",
    "    if loss[1] == KeyboardInterrupt:\n",
    "        raise KeyboardInterrupt\n",
    "    \n",
    "    accuracy = test(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        device=device,\n",
    "        data_loader=test_loader,\n",
    "        loss_function=F.mse_loss,\n",
    "        gt_data=gt_speed[\"Tractor01\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Testing 4180/4202, estimated MAPE 37.25%          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2346810/3149964060.py:26: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  test_loss.append(loss_function(output, y).item())\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (20) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2346810/1789720636.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m accuracy = test(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2346810/3149964060.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch, model, device, data_loader, loss_function, gt_data)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Calculate loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtest_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3789\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m     return torch._C._nn.mse_loss(\n\u001b[1;32m   3793\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (20) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "accuracy = test(\n",
    "    epoch=1,\n",
    "    model=model,\n",
    "    device=device,\n",
    "    data_loader=test_loader,\n",
    "    loss_function=F.mse_loss,\n",
    "    gt_data=gt_speed[\"Tractor01\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2346810/3469749070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
