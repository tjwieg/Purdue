@InProceedings{Marban_EstimatingPositionVelocity_2017,
  author    = {Marban, Arturo and Srinivasan, Vignesh and Samek, Wojciech and Fern{\'a}ndez, Josep and Casals, Alicia},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision Workshops},
  title     = {Estimating position and velocity in 3d space from monocular video sequences using a deep neural network},
  year      = {2017},
  pages     = {1460--1469},
}

@Article{Bell_AccurateVehicleSpeed_2020,
  author    = {Bell, D and Xiao, W and James, P},
  journal   = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title     = {Accurate vehicle speed estimation from monocular camera footage},
  year      = {2020},
  pages     = {419--426},
  volume    = {2},
  publisher = {Copernicus Publications G{\"o}ttingen, Germany},
}

@Article{GarciaAguilar_DetectionDangerouslyApproaching_2024,
  author   = {García-Aguilar, Iván and García-González, Jorge and Medina, Daniel and Luque-Baena, Rafael Marcos and Domínguez, Enrique and López-Rubio, Ezequiel},
  journal  = {Neurocomputing},
  title    = {Detection of dangerously approaching vehicles over onboard cameras by speed estimation from apparent size},
  year     = {2024},
  issn     = {0925-2312},
  month    = jan,
  pages    = {127057},
  volume   = {567},
  abstract = {Autonomous driving requires information such as the velocity of other vehicles to prevent potential hazards. This work proposes a real-time deep learning-based framework to estimate vehicle speeds from image captures through an onboard camera. Vehicles are detected and tracked by the proposed deep neural networks and a tracking algorithm, which analyzes the trajectories. Finally, a linear regression model estimates the speed of a vehicle based on its position and size in the camera frame. This proposal has been tested with two sequences of the Prevention dataset with satisfactory results. The system can estimate the speed of multiple vehicles simultaneously. It can be integrated easily with onboard computer systems, thus allowing to development of a low-cost solution for speed estimation in an everyday vehicle. The potential applications include vehicle safety systems, driver assistance, and autonomous driving technologies.},
  doi      = {10.1016/j.neucom.2023.127057},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0925231223011803/pdfft?download=true:application/pdf},
  keywords = {Object detection, Speed estimation, Deep learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231223011803},
  urldate  = {2024-10-08},
}

@Article{Hendrickx_AiBasedTracking_2024,
  author    = {Hendrickx, Hanne and Blanch, Xabier and Elias, Melanie and Delaloye, Reynald and Eltner, Anette},
  journal   = {EGUsphere},
  title     = {{AI}-{Based} {Tracking} of {Fast}-{Moving} {Alpine} {Landforms} {Using} {High} {Frequency} {Monoscopic} {Time}-{Lapse} {Imagery}},
  year      = {2024},
  month     = aug,
  pages     = {1--20},
  abstract  = {Active rock glaciers and landslides are critical indicators of permafrost dynamics in high mountain environments, reflecting the thermal state of permafrost and responding sensitively to climate change. Traditional monitoring methods, such as Global Navigation Satellite System (GNSS) measurements and permanent installations, face challenges in measuring the rapid movements of these landforms due to environmental constraints and limited spatial coverage. Remote sensing techniques offer improved spatial resolution but often lack the necessary temporal resolution to capture sub-seasonal variations. In this study, we introduce a novel approach utilising monoscopic time-lapse imagery and Artificial Intelligence (AI) for high-temporal-resolution velocity estimation, applied to two subsets of time-lapse datasets capturing a fast-moving landslide and rock glacier at the Grabengufer site (Swiss Alps). Specifically, we employed the Persistent Independent Particle tracking (PIPs++) model for tracking and the AI-based LightGlue matching algorithm to transfer 2D image data into 3D object space and further into 4D velocity data. This methodology was validated against GNSS surveys, demonstrating its capability to provide spatially and temporally detailed velocity information. Our findings highlight the potential of image-driven methodologies to enhance the understanding of dynamic landform processes, revealing spatio-temporal patterns previously unattainable with conventional monitoring techniques. By leveraging existing time-lapse data, our method offers a cost-effective solution for monitoring various geohazards, from rock glaciers to landslides, with implications for enhancing alpine safety and informing climate change impacts on permafrost dynamics. This study marks the pioneering application of AI-based methodologies in environmental monitoring using time-lapse image data, promising advancements in both research and practical applications within geomorphic studies.},
  doi       = {10.5194/egusphere-2024-2570},
  file      = {Full Text PDF:https\://egusphere.copernicus.org/preprints/2024/egusphere-2024-2570/egusphere-2024-2570.pdf:application/pdf},
  language  = {English},
  publisher = {Copernicus GmbH},
  url       = {https://egusphere.copernicus.org/preprints/2024/egusphere-2024-2570/},
  urldate   = {2024-10-08},
}

@Article{FernandezLlorca_VisionBasedVehicle_2021,
  author     = {Fernández Llorca, David and Hernández Martínez, Antonio and García Daza, Iván},
  journal    = {IET Intelligent Transport Systems},
  title      = {Vision-based vehicle speed estimation: {A} survey},
  year       = {2021},
  issn       = {1751-9578},
  number     = {8},
  pages      = {987--1005},
  volume     = {15},
  abstract   = {The need to accurately estimate the speed of road vehicles is becoming increasingly important for at least two main reasons. First, the number of speed cameras installed worldwide has been growing in recent years, as the introduction and enforcement of appropriate speed limits are considered one of the most effective means to increase the road safety. Second, traffic monitoring and forecasting in road networks plays a fundamental role to enhance traffic, emissions and energy consumption in smart cities, being the speed of the vehicles one of the most relevant parameters of the traffic state. Among the technologies available for the accurate detection of vehicle speed, the use of vision-based systems brings great challenges to be solved, but also great potential advantages, such as the drastic reduction of costs due to the absence of expensive range sensors, and the possibility of identifying vehicles accurately. This paper provides a review of vision-based vehicle speed estimation. The terminology and the application domains are described and a complete taxonomy of a large selection of works that categorizes all stages involved is proposed. An overview of performance evaluation metrics and available datasets is provided. Finally, current limitations and future directions are discussed.},
  copyright  = {© 2021 The Authors. IET Intelligent Transport Systems published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology},
  doi        = {10.1049/itr2.12079},
  file       = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1049/itr2.12079:application/pdf},
  keywords   = {Optical, image and video signal processing, Velocity, acceleration and rotation measurement, Computer vision and image processing techniques, Traffic engineering computing},
  language   = {en},
  shorttitle = {Vision-based vehicle speed estimation},
  urldate    = {2024-10-08},
}

@Article{Nister_VisualOdometryGround_2006,
  author    = {Nistér, David and Naroditsky, Oleg and Bergen, James},
  journal   = {Journal of Field Robotics},
  title     = {Visual odometry for ground vehicle applications},
  year      = {2006},
  issn      = {1556-4967},
  number    = {1},
  pages     = {3--20},
  volume    = {23},
  abstract  = {We present a system that estimates the motion of a stereo head, or a single moving camera, based on video input. The system operates in real time with low delay, and the motion estimates are used for navigational purposes. The front end of the system is a feature tracker. Point features are matched between pairs of frames and linked into image trajectories at video rate. Robust estimates of the camera motion are then produced from the feature tracks using a geometric hypothesize-and-test architecture. This generates motion estimates from visual input alone. No prior knowledge of the scene or the motion is necessary. The visual estimates can also be used in conjunction with information from other sources, such as a global positioning system, inertia sensors, wheel encoders, etc. The pose estimation method has been applied successfully to video from aerial, automotive, and handheld platforms. We focus on results obtained with a stereo head mounted on an autonomous ground vehicle. We give examples of camera trajectories estimated in real time purely from images over previously unseen distances (600 m) and periods of time. © 2006 Wiley Periodicals, Inc.},
  copyright = {Copyright © 2006 John Wiley \& Sons, Inc.},
  doi       = {10.1002/rob.20103},
  file      = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/rob.20103:application/pdf},
  language  = {en},
  urldate   = {2024-10-08},
}

@InProceedings{Howard_RealTimeStereo_2008,
  author    = {Howard, Andrew},
  booktitle = {2008 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
  title     = {Real-time stereo visual odometry for autonomous ground vehicles},
  year      = {2008},
  month     = sep,
  note      = {ISSN: 2153-0866},
  pages     = {3946--3952},
  abstract  = {This paper describes a visual odometry algorithm for estimating frame-to-frame camera motion from successive stereo image pairs. The algorithm differs from most visual odometry algorithms in two key respects: (1) it makes no prior assumptions about camera motion, and (2) it operates on dense disparity images computed by a separate stereo algorithm. This algorithm has been tested on many platforms, including wheeled and legged vehicles, and has proven to be fast, accurate and robust. For example, after 4000 frames and 400m of travel, position errors are typically less than 1m (0.25\% of distance traveled). Processing time is approximately 20ms on a 512x384 image. This paper includes a detailed description of the algorithm and experimental evaluation on a variety of platforms and terrain types.},
  doi       = {10.1109/IROS.2008.4651147},
  file      = {Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=4651147&ref=:application/pdf},
  issn      = {2153-0866},
  keywords  = {Visualization, Cameras, Feature extraction, Heuristic algorithms, Robots, Distance measurement, Wheels},
  url       = {https://ieeexplore.ieee.org/abstract/document/4651147},
  urldate   = {2024-10-08},
}

@Article{Aqel_ReviewVisualOdometry_2016,
  author     = {Aqel, Mohammad O. A. and Marhaban, Mohammad H. and Saripan, M. Iqbal and Ismail, Napsiah Bt.},
  journal    = {SpringerPlus},
  title      = {Review of visual odometry: types, approaches, challenges, and applications},
  year       = {2016},
  issn       = {2193-1801},
  month      = oct,
  number     = {1},
  pages      = {1897},
  volume     = {5},
  abstract   = {Accurate localization of a vehicle is a fundamental challenge and one of the most important tasks of mobile robots. For autonomous navigation, motion tracking, and obstacle detection and avoidance, a robot must maintain knowledge of its position over time. Vision-based odometry is a robust technique utilized for this purpose. It allows a vehicle to localize itself robustly by using only a stream of images captured by a camera attached to the vehicle. This paper presents a review of state-of-the-art visual odometry (VO) and its types, approaches, applications, and challenges. VO is compared with the most common localization sensors and techniques, such as inertial navigation systems, global positioning systems, and laser sensors. Several areas for future research are also highlighted.},
  doi        = {10.1186/s40064-016-3573-7},
  file       = {Full Text PDF:https\://link.springer.com/content/pdf/10.1186%2Fs40064-016-3573-7.pdf:application/pdf},
  keywords   = {Artificial Intelligence, Visual odometry, Localization sensors, Image stream, Global positioning system, Inertial navigation system},
  language   = {en},
  shorttitle = {Review of visual odometry},
  urldate    = {2024-10-08},
}

@Misc{Wang_TemporalSegmentNetworks_2016,
  author        = {Limin Wang and Yuanjun Xiong and Zhe Wang and Yu Qiao and Dahua Lin and Xiaoou Tang and Luc Van Gool},
  title         = {Temporal Segment Networks: Towards Good Practices for Deep Action Recognition},
  year          = {2016},
  archiveprefix = {arXiv},
  doi           = {10.48550/arxiv.1608.00859},
  eprint        = {1608.00859},
  eprinttype    = {arxiv},
  primaryclass  = {cs.CV},
}

@Article{Yang_DepthAnythingV2_2024,
  author     = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  title      = {Depth Anything V2},
  year       = {2024},
  eprint     = {2406.09414},
  eprinttype = {arxiv},
}

@Article{Balmos_IsoblueAvenaFramework_2022,
  author     = {Balmos, Andrew D. and Castiblanco, Fabio A. and Neustedter, Aaron J. and Krogmeier, James V. and Buckmaster, Dennis R.},
  journal    = {IEEE Micro},
  title      = {{ISOBlue} {Avena}: {A} {Framework} for {Agricultural} {Edge} {Computing} and {Data} {Sovereignty}},
  year       = {2022},
  issn       = {1937-4143},
  month      = jan,
  number     = {1},
  pages      = {78--86},
  volume     = {42},
  doi        = {10.1109/MM.2021.3134830},
  file       = {IEEE Xplore Full Text PDF:https\://ieeexplore.ieee.org/ielx7/40/9705173/09705189.pdf?tp=&arnumber=9705189&isnumber=9705173&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk3MDUxODk=:application/pdf},
  shorttitle = {{ISOBlue} {Avena}},
  url        = {https://ieeexplore.ieee.org/abstract/document/9705189},
  urldate    = {2024-08-16},
}

@Misc{OatsCenter_IsoblueHardwareAvena_2023,
  author       = {Oats-Center},
  howpublished = {Online},
  title        = {{ISOBlue} Hardware, Avena software, and Deployment Files},
  year         = {2023},
  url          = {https://github.com/oats-center/isoblue},
}

@Software{Newman_GoproLabs_2024,
  author  = {David Newman},
  license = {Apache-2.0},
  title   = {{GoPro} {Labs}},
  url     = {https://github.com/gopro/labs},
  urldate = {2024-09-03},
  version = {2.51.75},
  year    = {2022},
}

@Software{Casillas_Gopro2gpx_2023,
  author  = {Juan M. Casillas},
  license = {GPL-3.0},
  title   = {gopro2gpx},
  url     = {https://github.com/juanmcasillas/gopro2gpx},
  urldate = {2024-09-03},
  version = {dff353a},
  year    = {2023},
}

@Comment{jabref-meta: databaseType:bibtex;}
