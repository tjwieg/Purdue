 
@Article{Silva.MethodEmbeddingComputer.2020,
  author   = {Silva, Elias T. and Sampaio, Fausto and da Silva, Lucas C. and Medeiros, David S. and Correia, Gustavo P.},
  journal  = {Microprocessors and Microsystems},
  title    = {A method for embedding a computer vision application into a wearable device},
  year     = {2020},
  issn     = {0141-9331},
  month    = jul,
  pages    = {103086},
  volume   = {76},
  abstract = {Pattern classification applications can be found everywhere, especially the ones that use computer vision. What makes them difficult to embed is the fact that they often require a lot of computational resources. Embedded computer vision has been applied in many contexts, such as industrial or home automation, robotics, and assistive technologies. This work performs a design space exploration in an image classification system and embeds a computer vision application into a minimum resource platform, targeting wearable devices. The feature extractor and the classifier are evaluated for memory usage and computation time. A method is proposed to optimize such characteristics, leading to a reduction of over 99\% in computation time and 92\% in memory usage, with respect to a standard implementation. Experimental results in an ARM Cortex-M platform showed a total classification time of 0.3 s, maintaining the same accuracy as in the simulation performed. Furthermore, less than 20 KB of data memory was required, which is the most limited resource available in low-cost and low-power microcontrollers. The target application, used for the experimental evaluation, is a crosswalk detector used to help visually impaired persons.},
  doi      = {10.1016/j.micpro.2020.103086},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/S0141933119304089/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords = {Embedded systems, Design space exploration, Computer vision, Pattern detection, Low-power wearable applications},
  url      = {https://www.sciencedirect.com/science/article/pii/S0141933119304089},
  urldate  = {2024-04-21},
}

 
@Article{Immonen.TinyMachineLearning.2022,
  author    = {Immonen, Riku and Hämäläinen, Timo},
  journal   = {Journal of Sensors},
  title     = {Tiny {Machine} {Learning} for {Resource}-{Constrained} {Microcontrollers}},
  year      = {2022},
  issn      = {1687-725X},
  month     = nov,
  pages     = {e7437023},
  volume    = {2022},
  abstract  = {We use 250 billion microcontrollers daily in electronic devices that are capable of running machine learning models inside them. Unfortunately, most of these microcontrollers are highly constrained in terms of computational resources, such as memory usage or clock speed. These are exactly the same resources that play a key role in teaching and running a machine learning model with a basic computer. However, in a microcontroller environment, constrained resources make a critical difference. Therefore, a new paradigm known as tiny machine learning had to be created to meet the constrained requirements of the embedded devices. In this review, we discuss the resource optimization challenges of tiny machine learning and different methods, such as quantization, pruning, and clustering, that can be used to overcome these resource difficulties. Furthermore, we summarize the present state of tiny machine learning frameworks, libraries, development environments, and tools. The benchmarking of tiny machine learning devices is another thing to be concerned about; these same constraints of the microcontrollers and diversity of hardware and software turn to benchmark challenges that must be resolved before it is possible to measure performance differences reliably between embedded devices. We also discuss emerging techniques and approaches to boost and expand the tiny machine learning process and improve data privacy and security. In the end, we form a conclusion about tiny machine learning and its future development.},
  doi       = {10.1155/2022/7437023},
  file      = {Full Text PDF:https\://downloads.hindawi.com/journals/js/2022/7437023.pdf:application/pdf},
  language  = {en},
  publisher = {Hindawi},
  url       = {https://www.hindawi.com/journals/js/2022/7437023/},
  urldate   = {2024-04-22},
}

 
@InProceedings{Falbo.AnalyzingMachineLearning.2020,
  author    = {Falbo, Vincenzo and Apicella, Tommaso and Aurioso, Daniele and Danese, Luisa and Bellotti, Francesco and Berta, Riccardo and De Gloria, Alessandro},
  booktitle = {Applications in {Electronics} {Pervading} {Industry}, {Environment} and {Society}},
  title     = {Analyzing {Machine} {Learning} on {Mainstream} {Microcontrollers}},
  year      = {2020},
  address   = {Cham},
  editor    = {Saponara, Sergio and De Gloria, Alessandro},
  pages     = {103--108},
  publisher = {Springer International Publishing},
  abstract  = {Machine learning in embedded systems has become a reality, with the first tools for neural network firmware development already being made available for ARM microcontroller developers. This paper explores the use of one of such tools, namely the STM X-Cube-AI, on mainstream ARM Cortex-M microcontrollers, analyzing their performance, and comparing support and performance of other two common supervised ML algorithms, namely Support Vector Machines (SVM) and k-Nearest Neighbours (k-NN). Results on three datasets show that X-Cube-AI provides quite constant good performance even with the limitations of the embedded platform. The workflow is well integrated with mainstream desktop tools, such as Tensorflow and Keras.},
  doi       = {10.1007/978-3-030-37277-4_12},
  file      = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2F978-3-030-37277-4_12.pdf:application/pdf},
  isbn      = {9783030372774},
  language  = {en},
}

 
@Article{Qasaimeh.BenchmarkingVisionKernels.2021,
  author   = {Qasaimeh, Murad and Denolf, Kristof and Khodamoradi, Alireza and Blott, Michaela and Lo, Jack and Halder, Lisa and Vissers, Kees and Zambreno, Joseph and Jones, Phillip H.},
  journal  = {Journal of Systems Architecture},
  title    = {Benchmarking vision kernels and neural network inference accelerators on embedded platforms},
  year     = {2021},
  issn     = {1383-7621},
  month    = feb,
  pages    = {101896},
  volume   = {113},
  abstract = {Developing efficient embedded vision applications requires exploring various algorithmic optimization trade-offs and a broad spectrum of hardware architecture choices. This makes navigating the solution space and finding the design points with optimal performance trade-offs a challenge for developers. To help provide a fair baseline comparison, we conducted comprehensive benchmarks of accuracy, run-time, and energy efficiency of a wide range of vision kernels and neural networks on multiple embedded platforms: ARM57 CPU, Nvidia Jetson TX2 GPU and Xilinx ZCU102 FPGA. Each platform utilizes their optimized libraries for vision kernels (OpenCV, VisionWorks and xfOpenCV) and neural networks (OpenCV DNN, TensorRT and Xilinx DPU). For vision kernels, our results show that the GPU achieves an energy/frame reduction ratio of 1.1–3.2× compared to the others for simple kernels. However, for more complicated kernels and complete vision pipelines, the FPGA outperforms the others with energy/frame reduction ratios of 1.2–22.3×. For neural networks [Inception-v2 and ResNet-50, ResNet-18, Mobilenet-v2 and SqueezeNet], it shows that the FPGA achieves a speed up of [2.5, 2.1, 2.6, 2.9 and 2.5]× and an EDP reduction ratio of [1.5, 1.1, 1.4, 2.4 and 1.7]× compared to the GPU FP16 implementations, respectively.},
  doi      = {10.1016/j.sysarc.2020.101896},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/S1383762120301697/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords = {Benchmarks, CPUs, GPUs, FPGAs, Embedded vision, Neural networks},
  url      = {https://www.sciencedirect.com/science/article/pii/S1383762120301697},
  urldate  = {2024-04-22},
}

 
@Article{Manoharan.EmbeddedImagingSystem.2020,
  author   = {Manoharan, Dr Samuel},
  journal  = {Journal of Electronics and Informatics},
  title    = {Embedded {Imaging} {System} {Based} {Behavior} {Analysis} of {Dairy} {Cow}},
  year     = {2020},
  issn     = {2582-3825},
  month    = jun,
  number   = {2},
  pages    = {148--154},
  volume   = {2},
  file     = {Full Text PDF:https\://irojournals.com/iroei/article/pdf/2/2/6:application/pdf},
  language = {en},
  url      = {https://irojournals.com/iroei/article/view/2/2/6},
  urldate  = {2024-04-22},
}

 
@Article{Lin.MachineVisionBased.2021,
  author   = {Lin, Jiaming and Du, Zihao and Yu, Chuying and Ge, Wenmin and Lü, Weichao and Deng, Huan and Zhang, Chao and Chen, Xiao and Zhang, Zejun and Xu, Jing},
  journal  = {Chinese Optics Letters},
  title    = {Machine-vision-based acquisition, pointing, and tracking system for underwater wireless optical communications},
  year     = {2021},
  issn     = {1671-7694},
  month    = mar,
  number   = {5},
  pages    = {050604},
  volume   = {19},
  abstract = {Researching (High Level Discipline Journal Cluster English Platform), previously known as CLP Publishing (the English version of Chinese Optics Journal, 2019) was launched in April, 2021, which provides the platform for publishing world-class journals independently...},
  doi      = {10.3788/COL202119.050604},
  file     = {Full Text PDF:https\://www.researching.cn/ArticlePdf/m00005/2021/19/5/050604.pdf:application/pdf},
  language = {EN},
  url      = {https://www.researching.cn/articles/OJe635ff4bb844bda0},
  urldate  = {2024-04-22},
}

 
@Article{Shamim.HardwareDeployableEdge.2022,
  author   = {Shamim, Mohammed Zubair M.},
  journal  = {IEEE Embedded Systems Letters},
  title    = {Hardware {Deployable} {Edge}-{AI} {Solution} for {Prescreening} of {Oral} {Tongue} {Lesions} {Using} {TinyML} on {Embedded} {Devices}},
  year     = {2022},
  issn     = {1943-0671},
  month    = dec,
  number   = {4},
  pages    = {183--186},
  volume   = {14},
  abstract = {Diagnosing oral cavity cancer (OCC) in its initial stages is an effective way to reduce patient mortality. However, current prescreening solutions are manual and the resultant clinical treatment is not cost effective for the average individual, primarily in developing nations. In this letter, we present an automated and inexpensive prescreening solution utilizing artificial intelligence (AI) deployed on embedded edge devices to detect benign and premalignant superficial oral tongue lesions. The proposed machine vision solution utilizes a clinically annotated photographic dataset of nine types of superficial oral tongue lesions to retrain a MobileNetV2 neural network using transfer learning. In this approach, we also utilized TensorFlow Lite for Microcontrollers to quantize a 32-bit floating point (float32) precision model into an 8-bit integer (int8) model for deployment on power and resource-constrained OpenMV Cam H7 Plus embedded edge device. The quantized int8 model was able to detect the nine tongue lesions with an accuracy of 98.69\% on the test set. More than 60\% reduction in on-device RAM and flash memory usage were measured for the int8 model when compared to the equivalently performing float32 model for relatively the same inference speed ( 1.1 ms) on the target edge device.},
  doi      = {10.1109/LES.2022.3160281},
  file     = {IEEE Xplore Full Text PDF:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=9737316&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzk3MzczMTY=:application/pdf},
  keywords = {Lesions, Tongue, Cancer, Computational modeling, Image edge detection, Training, Performance evaluation, Artificial intelligence (AI), embedded edge devices, oral cavity cancer (OCC), tiny machine learning (TinyML), tongue lesions},
  url      = {https://ieeexplore.ieee.org/abstract/document/9737316},
  urldate  = {2024-04-22},
}

 
@Book{Szeliski.ComputerVisionAlgorithms.2022,
  author     = {Szeliski, Richard},
  publisher  = {Springer Nature},
  title      = {Computer {Vision}: {Algorithms} and {Applications}},
  year       = {2022},
  isbn       = {9783030343729},
  month      = jan,
  note       = {Google-Books-ID: QptXEAAAQBAJ},
  abstract   = {Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a two-year old remains elusive. Why is computer vision such a challenging problem and what is the current state of the art? Computer Vision: Algorithms and Applications explores the variety of techniques commonly used to analyze and interpret images. It also describes challenging real-world applications where vision is being successfully used, both for specialized applications such as medical imaging, and for fun, consumer-level tasks such as image editing and stitching, which students can apply to their own personal photos and videos. More than just a source of “recipes,” this exceptionally authoritative and comprehensive textbook/reference also takes a scientific approach to basic vision problems, formulating physical models of the imaging process before inverting them to produce descriptions of a scene. These problems are also analyzed using statistical models and solved using rigorous engineering techniques Topics and features:  Structured to support active curricula and project-oriented courses, with tips in the Introduction for using the book in a variety of customized courses Presents exercises at the end of each chapter with a heavy emphasis on testing algorithms and containing numerous suggestions for small mid-term projects Provides additional material and more detailed mathematical topics in the Appendices, which cover linear algebra, numerical techniques, and Bayesian estimation theory Suggests additional reading at the end of each chapter, including the latest research in each sub-field, in addition to a full Bibliography at the end of the book Supplies supplementary course material for students at the associated website, http://szeliski.org/Book/  Suitable for an upper-level undergraduate or graduate-level course in computer science or engineering, this textbook focuses on basic techniques that work under real-world conditions and encourages students to push their creative boundaries. Its design and exposition also make it eminently suitable as a unique reference to the fundamental techniques and current research literature in computer vision.},
  file       = {Google Books Link:https\://books.google.com/books?id=QptXEAAAQBAJ:text/html},
  keywords   = {Computers / Software Development \& Engineering / Computer Graphics, Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition, Computers / Artificial Intelligence / General, Technology \& Engineering / Electronics / General, Technology \& Engineering / Materials Science / General, Computers / Optical Data Processing, Computers / Software Development \& Engineering / General, Technology \& Engineering / Imaging Systems},
  language   = {en},
  shorttitle = {Computer {Vision}},
}

 
@Article{Smith.QuietRevolutionMachine.2021,
  author   = {Smith, Melvyn L. and Smith, Lyndon N. and Hansen, Mark F.},
  journal  = {Computers in Industry},
  title    = {The quiet revolution in machine vision - a state-of-the-art survey paper, including historical review, perspectives, and future directions},
  year     = {2021},
  issn     = {0166-3615},
  month    = sep,
  pages    = {103472},
  volume   = {130},
  abstract = {Over the past few years, what might not unreasonably be described as a true revolution has taken place in the field of machine vision, radically altering the way many things had previously been done and offering new and exciting opportunities for those able to quickly embrace and master the new techniques. Rapid developments in machine learning, largely enabled by faster GPU-equipped computing hardware, has facilitated an explosion of machine vision applications into hitherto extremely challenging or, in many cases, previously impossible to automate industrial tasks. Together with developments towards an internet of things and the availability of big data, these form key components of what many consider to be the fourth industrial revolution. This transformation has dramatically improved the efficacy of some existing machine vision activities, such as in manufacturing (e.g. inspection for quality control and quality assurance), security (e.g. facial biometrics) and in medicine (e.g. detecting cancers), while in other cases has opened up completely new areas of use, such as in agriculture and construction (as well as in the existing domains of manufacturing and medicine). Here we will explore the history and nature of this change, what underlies it, what enables it, and the impact it has had - the latter by reviewing several recent indicative applications described in the research literature. We will also consider the continuing role that traditional or classical machine vision might still play. Finally, the key future challenges and developing opportunities in machine vision will also be discussed.},
  doi      = {10.1016/j.compind.2021.103472},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/S0166361521000798/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords = {Machine vision, Machine learning, Deep learning, State-of-the-art},
  url      = {https://www.sciencedirect.com/science/article/pii/S0166361521000798},
  urldate  = {2024-04-22},
}

 
@Article{Bayoudh.SurveyDeepMultimodal.2022,
  author     = {Bayoudh, Khaled and Knani, Raja and Hamdaoui, Fayçal and Mtibaa, Abdellatif},
  journal    = {The Visual Computer},
  title      = {A survey on deep multimodal learning for computer vision: advances, trends, applications, and datasets},
  year       = {2022},
  issn       = {1432-2315},
  month      = aug,
  number     = {8},
  pages      = {2939--2970},
  volume     = {38},
  abstract   = {The research progress in multimodal learning has grown rapidly over the last decade in several areas, especially in computer vision. The growing potential of multimodal data streams and deep learning algorithms has contributed to the increasing universality of deep multimodal learning. This involves the development of models capable of processing and analyzing the multimodal information uniformly. Unstructured real-world data can inherently take many forms, also known as modalities, often including visual and textual content. Extracting relevant patterns from this kind of data is still a motivating goal for researchers in deep learning. In this paper, we seek to improve the understanding of key concepts and algorithms of deep multimodal learning for the computer vision community by exploring how to generate deep models that consider the integration and combination of heterogeneous visual cues across sensory modalities. In particular, we summarize six perspectives from the current literature on deep multimodal learning, namely: multimodal data representation, multimodal fusion (i.e., both traditional and deep learning-based schemes), multitask learning, multimodal alignment, multimodal transfer learning, and zero-shot learning. We also survey current multimodal applications and present a collection of benchmark datasets for solving problems in various vision domains. Finally, we highlight the limitations and challenges of deep multimodal learning and provide insights and directions for future research.},
  doi        = {10.1007/s00371-021-02166-7},
  file       = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2Fs00371-021-02166-7.pdf:application/pdf},
  keywords   = {Applications, Computer vision, Datasets, Deep learning, Sensory modalities, Multimodal learning},
  language   = {en},
  shorttitle = {A survey on deep multimodal learning for computer vision},
  url        = {https://doi.org/10.1007/s00371-021-02166-7},
  urldate    = {2024-04-22},
}

 
@InProceedings{Liberis.uNasConstrainedNeural.2021,
  author     = {Liberis, Edgar and Dudziak, Łukasz and Lane, Nicholas D.},
  booktitle  = {Proceedings of the 1st {Workshop} on {Machine} {Learning} and {Systems}},
  title      = {Micro-{NAS}: {Constrained} {Neural} {Architecture} {Search} for {Microcontrollers}},
  year       = {2021},
  address    = {New York, NY, USA},
  month      = apr,
  pages      = {70--79},
  publisher  = {Association for Computing Machinery},
  series     = {{EuroMLSys} '21},
  abstract   = {IoT devices are powered by microcontroller units (MCUs) which are extremely resource-scarce: a typical MCU may have an underpowered processor and around 64 KB of memory and persistent storage. Designing neural networks for such a platform requires an intricate balance between keeping high predictive performance (accuracy) while achieving low memory and storage usage and inference latency. This is extremely challenging to achieve manually, so in this work, we build a neural architecture search (NAS) system, called µNAS, to automate the design of such small-yet-powerful MCU-level networks. µNAS explicitly targets the three primary aspects of resource scarcity of MCUs: the size of RAM, persistent storage and processor speed. µNAS represents a significant advance in resource-efficient models, especially for "mid-tier" MCUs with memory requirements ranging from 0.5 KB to 64 KB. We show that on a variety of image classification datasets µNAS is able to (a) improve top-1 classification accuracy by up to 4.8\%, or (b) reduce memory footprint by 4-13×, or (c) reduce the number of multiply-accumulate operations by at least 2×, compared to existing MCU specialist literature and resource-efficient models. µNAS is freely available for download at https://github.com/eliberis/uNAS},
  doi        = {10.1145/3437984.3458836},
  file       = {Full Text PDF:https\://dl.acm.org/doi/pdf/10.1145/3437984.3458836:application/pdf},
  isbn       = {9781450382984},
  keywords   = {NAS, microcontrollers, tinyml},
  shorttitle = {µ{NAS}},
  url        = {https://dl.acm.org/doi/10.1145/3437984.3458836},
  urldate    = {2024-04-22},
}

@Comment{jabref-meta: databaseType:bibtex;}
